%scala
//=============================Load files/tables into dataframe=========================================
//Read Json, TextFile, Table into Dataframe
//Load CSV file with header and comma delimited
scala> var df=spark.read.option("header", true).option("delimiter", ",").csv("file:///Downloads/test_file.csv")
df: org.apache.spark.sql.DataFrame = [x: string, y: string ... 36 more fields]

//Load the same CSV file with load option.
scala> var df=spark.read.format("csv").option("header", true).option("delimiter", ",").load("file:///Downloads/test_file.csv")
df: org.apache.spark.sql.DataFrame = [x: string, y: string ... 36 more fields]

//Load JSON file into Dataframe
scala> val df=spark.read.json("file:///test.json")
df: org.apache.spark.sql.DataFrame = [x: string, y: string ... 36 more fields]

//Load table into Dataframe
scala> val op=sqlContext.sql("select * from schema.table limit 5")
op: org.apache.spark.sql.DataFrame = [x: string, y: string ... 36 more fields]

//Read the Hive table
spark.read.table("default.test")


%scala
//=============================Write dataframe into files/tables =========================================
//Write Dataframe into CSV file with header and pipe delimited
scala> import org.apache.spark.sql.SaveMode
scala> df.write.mode(SaveMode.Overwrite).option("delimiter","|").format("csv").save("file:///Downloads/test_file1.csv")

//Write Dataframe into JSON file
scala> df.write.mode(SaveMode.Overwrite).format("json").save("file:///Downloads/test_file2")

//Write DAtaframe to file with compression
scala> df.write.mode(SaveMode.Overwrite).format("json").codec("gzip").save("file:///Downloads/test_file3")

//Write dataframe to file in parquet.
scala> df.write.mode(SaveMode.Overwrite).option("delimiter","|").format("csv").parquet("file:///Downloads/test_file4")

//Write output to single files
scala> df.coalesce(1).write.option("header",true).csv("file:///path.csv")

//Rename the file in hdfs
import org.apache.hadoop.fs._
val fs=FileSystem.get(spark.sparkContext.hadoopConfiguration)
val file= fs.globStatus(new Path("outputpath/part*")")(0).getPath().getName()   --> get the filename
fs.rename(new Path("path/" + file),new Path("newfilepath/file.csv"))



%scala
//=============================Convert RDD to Dataframe using ==============================================
//Using CASE CLASS
case class My_Schema (x:Int,y: Double,z:String,t:String)
val input=sc.textFile("file:///rdd_df_conversion")
val input1=input.map(x=>x.split(","))
val input1RDD = input1.map ( x=> My_Schema (x(0).toInt, x(1).toDouble, x(2).toString(),x(3).toString))
val inputDF=input1RDD.toDF()

//Using STRUCTFIELD
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{StructType, StructField, StringType,IntegerType,DoubleType};
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val My_Schema1 = new StructType(Array(
  StructField("x",IntegerType,nullable = true),
  StructField("y",DoubleType,nullable = true),
  StructField("z",StringType,nullable=true),
  StructField("t",StringType,nullable=true)
))

scala> val rowRDD = input.map(x=>x.split(",")).map(x â‡’ Row(x(0).trim.toInt, x(1).toDouble, x(2).toString(),x(3).toString()))
rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[174] at map at <console>:29

scala> val inputDF = sqlContext.createDataFrame(rowRDD,My_Schema1)
inputDF: org.apache.spark.sql.DataFrame = [x: int, y: double ... 2 more fields]


%scala
//=============================Useful Dataframe Operations ==============================================
//Create empty DF
val emptyDF = SparkSession.builder.getOrCreate.emptyDataFrame
val emptyDF = spark.emptyDataFrame

//Change column order in Dataframe
 val newinputDF=inputDF.select("y","x","t","z")
 val cols = df1.columns
 val dforder = df.select(cols.head,cols.tail:_*)
 val reordercol:Array[String]=Array("col3","col2","col1")
 df.select(reordercol.head,reordercol.tail:_*)



//Print the schema of the dataframe
scala> newinputDF.printSchema()

//Change datatype of required columns columns. Though the order is different in below command final DF will not change the structure.
var newinputDF1=inputDF.withColumn("z", 'z.cast(StringType))
var newinputDF1=inputDF.withColumn("z",$"z".cast(StringType))

//Add columns to the dataframe. lit will add constant value to all the rows in dataframe
import org.apache.spark.sql.functions._
inputDF.withColumn("D", lit(750))
inputDF.withColumn("D", lit(""))   -> create the empty columns in DF

//Rename the columns in DataFrame
val df = testdf.withColumnRenamed("oldcolumnsname","newcolumnsname")

//number of records in DF
df.count()

//filter
df.filter("store_nbr is not null")

//remove other than alpha numeric in newcolumnsname
df.withColumn("test",functions.regexp_replace(df.col("test"),"[^A-Za-z0-9]",""))

//Group By & aggregation
df.groupBy("col1","col2").agg(count($"col3").alias("itemcount"))

//Pivot SUM
df.groupby("Footage").pivot("CLUSTER").sum("itemcount")

//Order By
df.orderBy("col1")

//Windowing functions
val partByCluster = Window.partitionBy($"CLUSTER").orderBy($"Footage").desc
val actstrcnt = sum("strcnt").over(partByCluster).as('cumulative_total)
val actstrcntcumlativesum = df.select('*,actstrcnt).orderBy('Cluster,'Footage.desc)

//Fill empty with 0
df.na.fill(0)

//Cumulative SUM
df.withColumn("Cumulativesum",df.map(c => col(c)).reduce((c1,c2) => c1 + c2)).orderBy($"Footage")

//Update value in DF columns
df.withColumn("test",
  when(col(test).equalTo("national"),"NATIONAL")
  .when(col(test).equalTo("nationaltest"),"NATIONALTEST")
  .otherwise(col(test)))

//join DF with different columns name
df.join(df1.drop("col1"),df("col1")===df1("col2") && df("col3")===df1("col3"),"left").drop("col10")
