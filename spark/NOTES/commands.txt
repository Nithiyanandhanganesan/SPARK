LOAD FILE INTO SPARK :
=============================

val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")    ==> load file from hfds into RDD
val weatherFileRDD = sc.wholeTextFiles("hdfs://localhost:9000/user/hduser/weather/1901")


val wordsFlatMap = words.flatMap(_.split("\\W+"))                     ==> Divide lines into words
val wordsMap = wordsFlatMap.map( w => (w,1))
val wordCount = wordsMap.reduceByKey( (a,b) => (a+b))
val wordCountSorted = wordCount.sortByKey(true)
wordCount.collect.foreach(println)
words.count
val firstElement = weatherRDD.first
val firstValue = firstElement._2

Spark provides complete support for Hadoop's InputFormat so anything that can be read by Hadoop can be read by Spark as well.
The default InputFormat is TextInputFormat. TextInputFormat takes the byte offset of a line as a key and the content of a line as a value.
Spark uses the sc.textFile method to read using TextInputFormat. It ignores the byte offset and creates an RDD of strings.
By default, Spark creates one partition for each InputSplit class, which roughly corresponds to one block.
As one partition cannot contain more than one block, having fewer partitions than blocks is not allowed.

Sometimes you need to load data in a specific format and TextInputFormat is not a good fit for that. Spark provides two methods for this purpose:

sparkContext.hadoopFile: This supports the old MapReduce API
sparkContext.newAPIHadoopFile: This supports the new MapReduce API

CUSTOM INPUTFORMAT:
-----------------------

import org.apache.hadoop.io.Text
import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat

val currencyFile = sc.newAPIHadoopFile("hdfs://localhost:9000/user/hduser/currency",classOf[KeyValueTextInputFormat],classOf[Text],classOf[Text])
val currencyRDD = currencyFile.map( t => (t._1.toString,t._2.toString))

LOAD DATA FROM CASSANDRA:
-----------------------------

HDFS is a good fit for sequence data access, it does not work well with random access. For example, HDFS will work well when your average file size 
is 100 MB and you want to read the whole file. If you frequently access the nth line in a file or some other part as a record, HDFS would be too slow.

Relational databases have traditionally provided a solution to that, providing low latency, random access, but they do not work well with big data.
NoSQL databases such as Cassandra fill the gap by providing relational database type access but in a distributed architecture on commodity servers.

In this recipe, we will load data from Cassandra as a Spark RDD. To make that happen Datastax, the company behind Cassandra,
has contributed spark-cassandra-connector. This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs back to Cassandra, and execute CQL queries.


cqlsh> CREATE KEYSPACE people WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };
cqlsh> create columnfamily person(id int primary key,first_name varchar,last_name varchar);

cqlsh> insert into person(id,first_name,last_name) values(1,'Barack','Obama');
cqlsh> insert into person(id,first_name,last_name) values(2,'Joe','Smith');

download the spark-cassandra-connector JAR to use directly with the Spark shell
wget http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.1.0/spark-cassandra-connector_2.10-1.2.0.jar

scala> sc.getConf.set("spark.cassandra.connection.host", "localhost")
scala> import com.datastax.spark.connector._

scala> val personRDD = sc.cassandraTable("people","person")
scala> personRDD.count
scala> personRDD.collect.foreach(println)

Cassandra can also be accessed through Spark SQL. It has a wrapper around SQLContext called CassandraSQLContext; let's load it:

scala> val cc = new org.apache.spark.sql.cassandra.CassandraSQLContext(sc)
scala> val p = cc.sql("select * from people.person")
